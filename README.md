# ðŸ” carbon-data-pipeline

[![Apache Kafka](https://img.shields.io/badge/Apache-Kafka-231F20?logo=apachekafka)](https://kafka.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache-Spark-E25A1C?logo=apachespark)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-336791?logo=postgresql)](https://postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](https://docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Real-time IoT data ingestion and streaming pipeline for continuous Scope 3 carbon accounting â€” replacing 12-month survey cycles with live emission telemetry.**
>
> ---
>
> ## ðŸ“‹ Overview
>
> **carbon-data-pipeline** is a production-grade event-streaming infrastructure that ingests IoT sensor data, ERP transactions, logistics telemetry, and supplier API feeds in real-time to maintain a continuously updated Scope 3 carbon accounting ledger.
>
> The fundamental problem with traditional Scope 3 accounting is temporal: by the time emission data is collected, validated, and reported, it is 12â€“18 months stale. Decarbonization interventions based on last year's data are flying blind. This pipeline solves the staleness problem by treating carbon data as a first-class event stream.
>
> Core capabilities:
>
> - **Real-time IoT ingestion** from factory sensors, smart meters, vehicle telematics, and logistics platforms
> - - **Event-driven carbon accounting** with sub-minute latency from emission event to ledger update
>   - - **Multi-source integration** across ERP systems (SAP, Oracle), logistics APIs (DHL, FedEx), and supplier portals
>     - - **Streaming Scope 3 computation** using configurable emission factor lookup at the event level
>       - - **Immutable audit ledger** built on Apache Kafka for regulatory-grade data lineage
>         - - **Scalable to billions of events** with horizontal scaling via Kubernetes
>          
>           - ---
>
> ## ðŸ—ï¸ Architecture Diagram
>
> ```
> â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
> â•‘         CARBON DATA PIPELINE â€” STREAMING ARCHITECTURE             â•‘
> â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
> â•‘                                                                   â•‘
> â•‘  DATA SOURCES (Real-time)                                         â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
> â•‘  â”‚  Factory   â”‚ â”‚  Vehicle   â”‚ â”‚  ERP/SAP   â”‚ â”‚   Supplier     â”‚ â•‘
> â•‘  â”‚  IoT Snrs  â”‚ â”‚  Telemtcs  â”‚ â”‚  PO Events â”‚ â”‚   API Feeds    â”‚ â•‘
> â•‘  â”‚  (MQTT)    â”‚ â”‚  (REST)    â”‚ â”‚  (Webhooks)â”‚ â”‚  (REST/SFTP)   â”‚ â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
> â•‘        â”‚              â”‚              â”‚                â”‚            â•‘
> â•‘        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â•‘
> â•‘                                      â”‚                             â•‘
> â•‘                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
> â•‘                     â”‚    INGESTION LAYER             â”‚              â•‘
> â•‘                     â”‚    Kafka Connect + Producers   â”‚              â•‘
> â•‘                     â”‚    â€¢ Schema Registry (Avro)    â”‚              â•‘
> â•‘                     â”‚    â€¢ Dead Letter Queue         â”‚              â•‘
> â•‘                     â”‚    â€¢ Backpressure control      â”‚              â•‘
> â•‘                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
> â•‘                                      â”‚                             â•‘
> â•‘  KAFKA TOPICS:                       â–¼                             â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
> â•‘  â”‚  raw.iot.energy  â”‚  raw.logistics  â”‚  raw.procurement       â”‚  â•‘
> â•‘  â”‚  raw.supplier    â”‚  raw.transport  â”‚  dlq.failed_events     â”‚  â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
> â•‘                                 â”‚                                   â•‘
> â•‘  STREAM PROCESSING              â–¼                                   â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
> â•‘  â”‚  Apache Spark Structured Streaming                          â”‚  â•‘
> â•‘  â”‚                                                             â”‚  â•‘
> â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â•‘
> â•‘  â”‚  â”‚  Emission Factor Lookup (broadcast join)             â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ Match event type â†’ GHG Protocol category          â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ Apply contextual EF (country, technology, year)   â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ Compute kgCO2e per event                          â”‚  â”‚  â•‘
> â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â•‘
> â•‘  â”‚                                                             â”‚  â•‘
> â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â•‘
> â•‘  â”‚  â”‚  Aggregation Windows                                 â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ 5-min tumbling: real-time monitoring              â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ 1-hour sliding: trend detection                   â”‚  â”‚  â•‘
> â•‘  â”‚  â”‚  â€¢ 24-hour daily: inventory accrual                  â”‚  â”‚  â•‘
> â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
> â•‘                                 â”‚                                   â•‘
> â•‘  STORAGE LAYER                  â–¼                                   â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
> â•‘  â”‚  PostgreSQL    â”‚   â”‚   Time-Series DB  â”‚   â”‚  Data Warehouse  â”‚ â•‘
> â•‘  â”‚  (Audit Ledgr) â”‚   â”‚   (TimescaleDB)   â”‚   â”‚  (Snowflake /   â”‚ â•‘
> â•‘  â”‚  â€¢ Immutable   â”‚   â”‚   â€¢ Dashboards    â”‚   â”‚   BigQuery)     â”‚ â•‘
> â•‘  â”‚  â€¢ Partitioned â”‚   â”‚   â€¢ Anomaly det.  â”‚   â”‚   â€¢ Annual rpt  â”‚ â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
> â•‘                                 â”‚                                   â•‘
> â•‘  OUTPUTS                        â–¼                                   â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
> â•‘  â”‚  Live Dashboard (Grafana) â”‚ API (FastAPI) â”‚ CDP/CSRD Reports â”‚  â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
> â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
> ```
>
> ---
>
> ## â— Problem Statement
>
> ### The Carbon Data Latency Crisis
>
> Enterprise Scope 3 accounting operates on a 12â€“18 month reporting cycle. Companies set decarbonization targets against data that is already stale before intervention can begin. The root cause is architectural: carbon data is treated as a periodic batch report rather than a continuous real-time signal.
>
> | Dimension | Batch Approach | Streaming Approach |
> |---|---|---|
> | **Data Freshness** | 12â€“18 months stale | Sub-minute latency |
> | **Anomaly Detection** | Post-hoc, annual | Real-time threshold alerts |
> | **Intervention Speed** | Next fiscal year | Same operational day |
> | **Data Sources** | Surveys + invoices | IoT + ERP + logistics live |
> | **Audit Trail** | Manual spreadsheets | Immutable Kafka log |
> | **Scalability** | Excel/VLOOKUP | Billions of events/day |
>
> > *"You cannot decarbonize a supply chain on a 12-month feedback loop. Real-time emission telemetry is the foundation of science-based action."*
> >
> > ---
> >
> > ## âœ… Solution Overview
> >
> > ### Event-Driven Carbon Accounting Architecture
> >
> > The pipeline treats every energy consumption reading, every purchase order creation, every logistics leg departure, and every supplier production event as an emission-relevant event that must be immediately classified, quantified, and recorded.
> >
> > **Ingestion Layer**
> > Apache Kafka Connect with pre-built connectors ingests data from MQTT brokers (factory IoT), REST APIs (logistics, supplier portals), SAP/Oracle CDC streams (ERP procurement events), and SFTP file drops (monthly supplier data). All events are schema-validated with Apache Avro and registered in the Schema Registry before entering the processing pipeline.
> >
> > **Stream Processing Layer**
> > Spark Structured Streaming jobs run continuously with micro-batch intervals of 30 seconds to 5 minutes depending on stream type. Each event undergoes emission factor lookup via a broadcast-joined reference table, Scope 3 category assignment, and kgCO2e computation. Windowed aggregations produce rolling inventory totals at supplier, facility, category, and organizational levels.
> >
> > **Storage and Serving Layer**
> > Processed emission records land in three stores: PostgreSQL (audit ledger with immutable append-only writes), TimescaleDB (time-series for dashboards and trend analysis), and a data warehouse (historical analytics and annual regulatory reporting). A FastAPI service layer exposes inventory data to downstream applications.
> >
> > ---
> >
> > ## ðŸ’» Code, Installation & Analysis
> >
> > ### Prerequisites
> >
> > | Requirement | Version |
> > |---|---|
> > | Docker & Docker Compose | 24.0+ |
> > | Python | 3.10+ |
> > | RAM | 16 GB minimum |
> > | Storage | 50 GB (for development data) |
> >
> > ### Quick Start with Docker
> >
> > ```bash
> > git clone https://github.com/virbahu/carbon-data-pipeline.git
> > cd carbon-data-pipeline
> >
> > # Start the full stack
> > docker-compose up -d
> >
> > # Verify all services are healthy
> > docker-compose ps
> >
> > # Services started:
> > # âœ“ Kafka (3 brokers)
> > # âœ“ Schema Registry
> > # âœ“ Kafka Connect
> > # âœ“ Apache Spark (1 master, 2 workers)
> > # âœ“ PostgreSQL 15
> > # âœ“ TimescaleDB
> > # âœ“ Grafana Dashboard
> > # âœ“ FastAPI emission service
> >
> > # Load demo data (IoT + procurement events)
> > python scripts/load_demo_data.py --events 10000 --duration 60
> > ```
> >
> > ### Producing Carbon Events
> >
> > ```python
> > from pipeline.producers import CarbonEventProducer, IoTEnergyEvent
> > from datetime import datetime
> >
> > producer = CarbonEventProducer(bootstrap_servers="localhost:9092")
> >
> > # Produce an energy consumption event (from factory smart meter)
> > event = IoTEnergyEvent(
> >     sensor_id="SM-PLANT-DE-042",
> >     facility_id="FACILITY_MUENCHEN_01",
> >     country_iso2="DE",
> >     energy_kwh=1247.3,
> >     energy_source="grid",
> >     grid_carbon_intensity_gco2_kwh=385.2,  # German grid, 2025
> >     timestamp=datetime.utcnow(),
> >     scope=2  # Direct measurement for Scope 2
> > )
> >
> > producer.send("raw.iot.energy", key=event.sensor_id, value=event)
> > print(f"Produced: {event.energy_kwh} kWh â†’ {event.energy_kwh * event.grid_carbon_intensity_gco2_kwh / 1e6:.2f} tCO2e")
> > ```
> >
> > ### Querying the Carbon Ledger
> >
> > ```python
> > from api.client import CarbonLedgerClient
> >
> > client = CarbonLedgerClient(base_url="http://localhost:8000")
> >
> > # Get real-time Scope 3 inventory for a supplier
> > inventory = client.get_supplier_inventory(
> >     supplier_id="SUP_042_DE",
> >     scope=3,
> >     start_date="2025-01-01",
> >     end_date="2025-12-31",
> >     granularity="daily"
> > )
> >
> > print(f"YTD Scope 3 (Supplier): {inventory.total_tco2e:,.1f} tCO2e")
> > print(f"Last updated: {inventory.last_event_timestamp}")
> > # >> YTD Scope 3 (Supplier): 4,832.7 tCO2e
> > # >> Last updated: 2025-12-20T14:32:07Z  (< 1 minute ago)
> > ```
> >
> > ---
> >
> > ## ðŸ“¦ Dependencies
> >
> > ```yaml
> > # docker-compose.yml services
> > services:
> >   kafka:
> >     image: confluentinc/cp-kafka:7.6.0
> >   schema-registry:
> >     image: confluentinc/cp-schema-registry:7.6.0
> >   kafka-connect:
> >     image: confluentinc/cp-kafka-connect:7.6.0
> >   spark-master:
> >     image: bitnami/spark:3.5
> >   spark-worker:
> >     image: bitnami/spark:3.5
> >   postgres:
> >     image: postgres:15
> >   timescaledb:
> >     image: timescale/timescaledb:latest-pg15
> >   grafana:
> >     image: grafana/grafana:10.3.0
> > ```
> >
> > ```toml
> > [tool.poetry.dependencies]
> > python = "^3.10"
> > confluent-kafka = "^2.3"
> > pyspark = "^3.5"
> > fastavro = "^1.9"
> > psycopg2-binary = "^2.9"
> > sqlalchemy = "^2.0"
> > fastapi = "^0.110"
> > pandas = "^2.0"
> > pydantic = "^2.0"
> > ```
> >
> > ---
> >
> > ## ðŸ‘¤ Author
> >
> > **Virbahu Jain** â€” Founder & CEO, [Quantisage](https://quantisage.com)
> >
> > > *Building the AI Operating System for Scope 3 emissions management and supply chain decarbonization.*
> > >
> > > | | |
> > > |---|---|
> > > | ðŸŽ“ **Education** | MBA, Kellogg School of Management, Northwestern University |
> > > | ðŸ­ **Experience** | 20+ years across manufacturing, life sciences, energy & public sector |
> > > | ðŸŒ **Scope** | Supply chain operations on five continents |
> > > | ðŸ“ **Research** | Peer-reviewed publications on AI in sustainable supply chains |
> > > | ðŸ”¬ **Patents** | IoT and AI solutions for manufacturing and logistics |
> > >
> > > [![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?logo=linkedin)](https://linkedin.com/in/virbahu)
> > > [![GitHub](https://img.shields.io/badge/GitHub-virbahu-181717?logo=github)](https://github.com/virbahu)
> > >
> > > ---
> > >
> > > ## ðŸ“„ License
> > >
> > > MIT License â€” see [LICENSE](LICENSE) for details.
> > >
> > > ---
> > >
> > > <div align="center">
<sub>Part of the <strong>Quantisage Open Source Initiative</strong> | AI Ã— Supply Chain Ã— Climate</sub>
</div>
