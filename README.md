# ğŸ” carbon-data-pipeline

[![Apache Kafka](https://img.shields.io/badge/Apache-Kafka-231F20?logo=apachekafka)](https://kafka.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache-Spark-E25A1C?logo=apachespark)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-336791?logo=postgresql)](https://postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](https://docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-Virbahu%20Jain-4285F4?logo=googlescholar&logoColor=white)](https://scholar.google.com/citations?user=4SN8o-QAAAAJ&hl=en)

> **Real-time IoT data ingestion and streaming pipeline for continuous Scope 3 carbon accounting â€” replacing 12-month survey cycles with live emission telemetry.**
>
> ---
>
> ## ğŸ“‹ Overview
>
> **carbon-data-pipeline** is a production-grade event-streaming infrastructure that ingests IoT sensor data, ERP transactions, logistics telemetry, and supplier API feeds in real-time to maintain a continuously updated Scope 3 carbon accounting ledger.
>
> The fundamental problem with traditional Scope 3 accounting is temporal: by the time emission data is collected, validated, and reported, it is 12â€“18 months stale. Decarbonization interventions based on last year's data are flying blind. This pipeline solves the staleness problem by treating carbon data as a first-class event stream.
>
> Core capabilities:
>
> - **Real-time IoT ingestion** from factory sensors, smart meters, vehicle telematics, and logistics platforms
> - - **Event-driven carbon accounting** with sub-minute latency from emission event to ledger update
>   - - **Multi-source integration** across ERP systems (SAP, Oracle), logistics APIs (DHL, FedEx), and supplier portals
>     - - **Streaming Scope 3 computation** using configurable emission factor lookup at the event level
>       - - **Immutable audit ledger** built on Apache Kafka for regulatory-grade data lineage
>         - - **Scalable to billions of events** with horizontal scaling via Kubernetes
>          
>           - ---
>
> ## ğŸ–¼ï¸ Data Flow Overview
>
> ![Streaming Architecture](https://img.shields.io/badge/IoT%20â†’%20Kafka%20â†’%20Spark%20â†’%20PostgreSQL-Real--time%20Carbon%20Ledger-orange?style=for-the-badge&logo=apachekafka)
>
> ```
>  IoT Sensors (MQTT)  â”€â”
>  ERP Events (CDC)    â”€â”€â”¼â”€â”€â–º Kafka Topics â”€â”€â–º Spark Streaming â”€â”€â–º Carbon Ledger
>  Logistics APIs      â”€â”€â”¤                            â”‚
>  Supplier Portals    â”€â”˜                             â–¼
>                                            Emission Factor Lookup
>                                            (broadcast join)
>                                                      â”‚
>                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>                                   â–¼                  â–¼              â–¼
>                              PostgreSQL         TimescaleDB    Data Warehouse
>                              (Audit Log)        (Dashboard)    (Annual Report)
> ```
>
> ---
>
> ## ğŸ—ï¸ Architecture Diagram
>
> ```
> â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
> â•‘         CARBON DATA PIPELINE â€” STREAMING ARCHITECTURE             â•‘
> â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
> â•‘                                                                   â•‘
> â•‘  DATA SOURCES (Real-time)                                         â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
> â•‘  â”‚  Factory   â”‚ â”‚  Vehicle   â”‚ â”‚  ERP/SAP   â”‚ â”‚   Supplier     â”‚ â•‘
> â•‘  â”‚  IoT Snrs  â”‚ â”‚  Telemtcs  â”‚ â”‚  PO Events â”‚ â”‚   API Feeds    â”‚ â•‘
> â•‘  â”‚  (MQTT)    â”‚ â”‚  (REST)    â”‚ â”‚  (Webhooks)â”‚ â”‚  (REST/SFTP)   â”‚ â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
> â•‘        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â•‘
> â•‘                                      â”‚                             â•‘
> â•‘                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
> â•‘                     â”‚    INGESTION LAYER             â”‚              â•‘
> â•‘                     â”‚    Kafka Connect + Producers   â”‚              â•‘
> â•‘                     â”‚    â€¢ Schema Registry (Avro)    â”‚              â•‘
> â•‘                     â”‚    â€¢ Dead Letter Queue         â”‚              â•‘
> â•‘                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
> â•‘                                      â”‚                             â•‘
> â•‘  KAFKA TOPICS:                       â–¼                             â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
> â•‘  â”‚  raw.iot.energy  â”‚  raw.logistics  â”‚  raw.procurement       â”‚  â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
> â•‘                                 â”‚                                   â•‘
> â•‘  STREAM PROCESSING              â–¼                                   â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
> â•‘  â”‚  Apache Spark Structured Streaming                          â”‚  â•‘
> â•‘  â”‚  â€¢ Emission Factor Lookup (broadcast join)                  â”‚  â•‘
> â•‘  â”‚  â€¢ GHG Protocol category assignment                         â”‚  â•‘
> â•‘  â”‚  â€¢ kgCO2e computation per event                             â”‚  â•‘
> â•‘  â”‚  â€¢ 5-min / 1-hour / 24-hour aggregation windows             â”‚  â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
> â•‘                                 â”‚                                   â•‘
> â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
> â•‘  â”‚  PostgreSQL    â”‚   â”‚   TimescaleDB     â”‚   â”‚  Data Warehouse  â”‚ â•‘
> â•‘  â”‚  (Audit Ledgr) â”‚   â”‚   (Dashboards)    â”‚   â”‚  (Annual rpt)   â”‚ â•‘
> â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
> â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
> ```
>
> ---
>
> ## â— Problem Statement
>
> ### The Carbon Data Latency Crisis
>
> Enterprise Scope 3 accounting operates on a 12â€“18 month reporting cycle. Companies set decarbonization targets against data that is already stale before intervention can begin.
>
> | Dimension | Batch Approach | Streaming Approach |
> |---|---|---|
> | **Data Freshness** | 12â€“18 months stale | Sub-minute latency |
> | **Anomaly Detection** | Post-hoc, annual | Real-time threshold alerts |
> | **Intervention Speed** | Next fiscal year | Same operational day |
> | **Data Sources** | Surveys + invoices | IoT + ERP + logistics live |
> | **Audit Trail** | Manual spreadsheets | Immutable Kafka log |
> | **Scalability** | Excel/VLOOKUP | Billions of events/day |
>
> > *"You cannot decarbonize a supply chain on a 12-month feedback loop. Real-time emission telemetry is the foundation of science-based action."*
> >
> > ---
> >
> > ## âœ… Solution Overview
> >
> > ### Event-Driven Carbon Accounting Architecture
> >
> > The pipeline treats every energy consumption reading, every purchase order creation, every logistics leg departure, and every supplier production event as an emission-relevant event that must be immediately classified, quantified, and recorded.
> >
> > **Ingestion Layer**
> > Apache Kafka Connect with pre-built connectors ingests data from MQTT brokers (factory IoT), REST APIs (logistics, supplier portals), SAP/Oracle CDC streams (ERP procurement events), and SFTP file drops. All events are schema-validated with Apache Avro and registered in the Schema Registry.
> >
> > **Stream Processing Layer**
> > Spark Structured Streaming jobs run continuously with micro-batch intervals of 30 seconds to 5 minutes. Each event undergoes emission factor lookup via a broadcast-joined reference table, Scope 3 category assignment, and kgCO2e computation. Windowed aggregations produce rolling inventory totals.
> >
> > **Storage and Serving Layer**
> > Processed emission records land in three stores: PostgreSQL (audit ledger), TimescaleDB (time-series for dashboards), and a data warehouse (historical analytics). A FastAPI service layer exposes inventory data to downstream applications.
> >
> > ---
> >
> > ## ğŸ’» Code, Installation & Analysis
> >
> > ### Quick Start with Docker
> >
> > ```bash
> > git clone https://github.com/virbahu/carbon-data-pipeline.git
> > cd carbon-data-pipeline
> >
> > # Start the full stack
> > docker-compose up -d
> >
> > # Services: Kafka, Schema Registry, Kafka Connect,
> > # Spark (1 master + 2 workers), PostgreSQL, TimescaleDB, Grafana, FastAPI
> >
> > # Load demo data
> > python scripts/load_demo_data.py --events 10000 --duration 60
> > ```
> >
> > ### Producing Carbon Events
> >
> > ```python
> > from pipeline.producers import CarbonEventProducer, IoTEnergyEvent
> > from datetime import datetime
> >
> > producer = CarbonEventProducer(bootstrap_servers="localhost:9092")
> >
> > event = IoTEnergyEvent(
> >     sensor_id="SM-PLANT-DE-042",
> >     facility_id="FACILITY_MUENCHEN_01",
> >     country_iso2="DE",
> >     energy_kwh=1247.3,
> >     energy_source="grid",
> >     grid_carbon_intensity_gco2_kwh=385.2,
> >     timestamp=datetime.utcnow()
> > )
> >
> > producer.send("raw.iot.energy", key=event.sensor_id, value=event)
> > ```
> >
> > ### Querying the Carbon Ledger
> >
> > ```python
> > from api.client import CarbonLedgerClient
> >
> > client = CarbonLedgerClient(base_url="http://localhost:8000")
> >
> > inventory = client.get_supplier_inventory(
> >     supplier_id="SUP_042_DE",
> >     scope=3,
> >     start_date="2025-01-01",
> >     end_date="2025-12-31"
> > )
> >
> > print(f"YTD Scope 3: {inventory.total_tco2e:,.1f} tCO2e")
> > print(f"Last updated: {inventory.last_event_timestamp}")
> > # >> YTD Scope 3: 4,832.7 tCO2e
> > # >> Last updated: 2025-12-20T14:32:07Z  (< 1 minute ago)
> > ```
> >
> > ---
> >
> > ## ğŸ“¦ Dependencies
> >
> > ```yaml
> > # docker-compose.yml
> > services:
> >   kafka:         confluentinc/cp-kafka:7.6.0
> >   schema-reg:    confluentinc/cp-schema-registry:7.6.0
> >   kafka-connect: confluentinc/cp-kafka-connect:7.6.0
> >   spark-master:  bitnami/spark:3.5
> >   spark-worker:  bitnami/spark:3.5
> >   postgres:      postgres:15
> >   timescaledb:   timescale/timescaledb:latest-pg15
> >   grafana:       grafana/grafana:10.3.0
> > ```
> >
> > ```toml
> > [tool.poetry.dependencies]
> > python = "^3.10"
> > confluent-kafka = "^2.3"
> > pyspark = "^3.5"
> > fastavro = "^1.9"
> > psycopg2-binary = "^2.9"
> > sqlalchemy = "^2.0"
> > fastapi = "^0.110"
> > pandas = "^2.0"
> > pydantic = "^2.0"
> > ```
> >
> > ---
> >
> > ## ğŸ‘¤ Author
> >
> > <img src="https://avatars.githubusercontent.com/u/virbahu" width="80" align="left" style="margin-right:15px; border-radius:50%"/>

**Virbahu Jain** â€” Founder & CEO, [Quantisage](https://quantisage.com)

> *Building the AI Operating System for Scope 3 emissions management and supply chain decarbonization.*
>
> <br clear="left"/>

| | |
|---|---|
| ğŸ“ **Education** | MBA, Kellogg School of Management, Northwestern University |
| ğŸ­ **Experience** | 20+ years across manufacturing, life sciences, energy & public sector |
| ğŸŒ **Scope** | Supply chain operations on five continents |
| ğŸ“ **Research** | Peer-reviewed publications on AI in sustainable supply chains |
| ğŸ”¬ **Patents** | IoT and AI solutions for manufacturing and logistics |

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?logo=linkedin)](https://linkedin.com/in/virbahu)
[![GitHub](https://img.shields.io/badge/GitHub-virbahu-181717?logo=github)](https://github.com/virbahu)
[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-Publications-4285F4?logo=googlescholar&logoColor=white)](https://scholar.google.com/citations?user=4SN8o-QAAAAJ&hl=en)
[![Quantisage](https://img.shields.io/badge/Company-Quantisage-00C853)](https://quantisage.com)

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

<div align="center">

![Quantisage](https://img.shields.io/badge/Quantisage-Open%20Source%20Initiative-00C853?style=for-the-badge)
![Supply Chain](https://img.shields.io/badge/AI-Supply%20Chain-blue?style=for-the-badge)
![Climate](https://img.shields.io/badge/Climate-Tech-green?style=for-the-badge)

<sub>Part of the <strong>Quantisage Open Source Initiative</strong> | AI Ã— Supply Chain Ã— Climate</sub>
</div>
